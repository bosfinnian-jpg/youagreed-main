<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Foundation</title>
    <link rel="icon" type="image/png" href="../assets/favicon2.png">
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&display=swap');

        :root {
            --primary: #000;
            --accent: #3b82f6;
            --accent-purple: #8b5cf6;
            --accent-pink: #ec4899;
            --accent-green: #10b981;
            --accent-orange: #f97316;
            --bg: #0a0a0a;
            --text: #ffffff;
            --text-secondary: #a1a1aa;
            
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --spacing-2xl: 4rem;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg);
            color: var(--text);
            overflow-x: hidden;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        html {
            scroll-behavior: smooth;
        }

        /* ANIMATED BACKGROUND */
        .cosmic-bg {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: -1;
            background: radial-gradient(ellipse at 20% 30%, rgba(59, 130, 246, 0.15) 0%, transparent 50%),
                        radial-gradient(ellipse at 80% 70%, rgba(139, 92, 246, 0.15) 0%, transparent 50%),
                        radial-gradient(ellipse at 50% 50%, rgba(236, 72, 153, 0.1) 0%, transparent 50%);
            animation: cosmicShift 20s ease-in-out infinite;
        }

        @keyframes cosmicShift {
            0%, 100% { transform: scale(1) rotate(0deg); }
            50% { transform: scale(1.1) rotate(5deg); }
        }

        /* FLOATING ORBS */
        .orbs-container {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: -1;
            pointer-events: none;
        }

        .orb {
            position: absolute;
            border-radius: 50%;
            filter: blur(60px);
            opacity: 0.3;
            will-change: transform;
            animation: float-orb 25s ease-in-out infinite;
        }

        .orb-1 {
            width: 300px;
            height: 300px;
            background: radial-gradient(circle, rgba(59, 130, 246, 0.6), transparent);
            top: 10%;
            left: 10%;
            animation-delay: 0s;
        }

        .orb-2 {
            width: 400px;
            height: 400px;
            background: radial-gradient(circle, rgba(139, 92, 246, 0.5), transparent);
            top: 50%;
            right: 10%;
            animation-delay: 5s;
        }

        .orb-3 {
            width: 350px;
            height: 350px;
            background: radial-gradient(circle, rgba(236, 72, 153, 0.4), transparent);
            bottom: 10%;
            left: 30%;
            animation-delay: 10s;
        }

        @keyframes float-orb {
            0%, 100% {
                transform: translate(0, 0) scale(1);
            }
            25% {
                transform: translate(50px, -30px) scale(1.1);
            }
            50% {
                transform: translate(-30px, 50px) scale(0.9);
            }
            75% {
                transform: translate(40px, 30px) scale(1.05);
            }
        }

        /* SCROLL INDICATOR */
        .scroll-indicator {
            position: fixed;
            top: 0;
            left: 0;
            height: 3px;
            background: linear-gradient(90deg, #667eea, #764ba2, #f093fb);
            z-index: 200;
            transition: width 0.1s ease;
        }

        /* HEADER */
        .site-header {
            background: rgba(10, 10, 10, 0.8);
            backdrop-filter: blur(20px);
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
            padding: var(--spacing-md) 0;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .header-content {
            max-width: 1600px;
            margin: 0 auto;
            padding: 0 var(--spacing-lg);
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .site-title {
            font-size: 1.3rem;
            font-weight: 700;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .nav-links {
            display: flex;
            gap: var(--spacing-md);
        }

        .nav-link {
            color: var(--text-secondary);
            text-decoration: none;
            font-size: 0.9rem;
            font-weight: 500;
            padding: 0.5rem 1rem;
            border-radius: 8px;
            transition: all 0.3s ease;
            border: 1px solid transparent;
        }

        .nav-link:hover {
            color: var(--text);
            background: rgba(139, 92, 246, 0.1);
            border-color: rgba(139, 92, 246, 0.3);
        }

        .nav-link.active {
            color: var(--accent-purple);
            background: rgba(139, 92, 246, 0.1);
            border-color: rgba(139, 92, 246, 0.3);
        }

        /* HERO SECTION */
        .hero-section {
            max-width: 1600px;
            margin: 0 auto;
            padding: 6rem var(--spacing-lg) 4rem;
            text-align: center;
        }

        .hero-title {
            font-size: clamp(3rem, 8vw, 7rem);
            font-weight: 900;
            letter-spacing: -0.03em;
            line-height: 1.1;
            margin-bottom: var(--spacing-md);
            background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #f093fb 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            animation: fadeInUp 1s ease;
        }

        .hero-subtitle {
            font-size: clamp(1rem, 2vw, 1.3rem);
            color: var(--text-secondary);
            margin-bottom: var(--spacing-xl);
            animation: fadeInUp 1s ease 0.2s both;
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        /* MAIN CONTAINER */
        .main-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 var(--spacing-lg) 4rem;
        }

        /* RESEARCH QUESTION CARD */
        .research-question-card {
            background: linear-gradient(135deg, rgba(59, 130, 246, 0.15), rgba(139, 92, 246, 0.15));
            backdrop-filter: blur(20px);
            border: 2px solid rgba(139, 92, 246, 0.4);
            border-radius: 24px;
            padding: var(--spacing-2xl);
            margin-bottom: var(--spacing-xl);
            position: relative;
            overflow: hidden;
            animation: fadeInUp 1s ease 0.4s both;
        }

        .research-question-card::before {
            content: '?';
            position: absolute;
            top: -20px;
            right: 20px;
            font-size: 12rem;
            font-weight: 900;
            color: rgba(139, 92, 246, 0.1);
            line-height: 1;
        }

        .research-question-label {
            font-size: 0.85rem;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.15em;
            color: var(--accent-purple);
            margin-bottom: var(--spacing-sm);
        }

        .research-question-text {
            font-size: clamp(1.5rem, 3vw, 2.5rem);
            font-weight: 700;
            line-height: 1.3;
            color: var(--text);
            position: relative;
            z-index: 1;
        }

        /* ARTICLE SECTION */
        .article-section {
            background: rgba(20, 20, 30, 0.6);
            backdrop-filter: blur(20px);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 24px;
            padding: var(--spacing-xl);
            margin-bottom: var(--spacing-lg);
            position: relative;
            overflow: hidden;
            animation: fadeInUp 1s ease both;
        }

        .article-section::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 4px;
            height: 100%;
            background: linear-gradient(180deg, var(--section-color-1), var(--section-color-2));
        }

        .article-header {
            display: flex;
            align-items: flex-start;
            gap: var(--spacing-md);
            margin-bottom: var(--spacing-lg);
            padding-left: var(--spacing-md);
        }

        .article-number {
            flex-shrink: 0;
            width: 60px;
            height: 60px;
            background: linear-gradient(135deg, var(--section-color-1), var(--section-color-2));
            border-radius: 16px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 700;
            font-size: 1.8rem;
            box-shadow: 0 5px 15px rgba(139, 92, 246, 0.4);
        }

        .article-meta {
            flex: 1;
        }

        .article-title {
            font-size: 1.5rem;
            font-weight: 800;
            color: var(--text);
            margin-bottom: 0.5rem;
            line-height: 1.3;
        }

        .article-details {
            font-size: 0.9rem;
            color: var(--text-secondary);
            line-height: 1.6;
        }

        .article-details strong {
            color: var(--text);
        }

        .article-content {
            padding-left: var(--spacing-md);
        }

        .article-content p {
            font-size: 1.05rem;
            line-height: 1.9;
            color: var(--text-secondary);
            margin-bottom: var(--spacing-md);
            text-align: justify;
        }

        .article-content p:last-child {
            margin-bottom: 0;
        }

        .article-content strong {
            color: var(--text);
            font-weight: 600;
        }

        .article-content em {
            color: var(--accent-purple);
            font-style: italic;
        }

        /* FOOTER */
        .site-footer {
            border-top: 1px solid rgba(255, 255, 255, 0.1);
            padding: var(--spacing-2xl) 0;
            text-align: center;
            color: var(--text-secondary);
            font-size: 0.95rem;
        }

        /* RESPONSIVE */
        @media (max-width: 768px) {
            .hero-section {
                padding: 3rem var(--spacing-md) 2rem;
            }

            .nav-links {
                gap: var(--spacing-xs);
            }

            .nav-link {
                font-size: 0.8rem;
                padding: 0.4rem 0.8rem;
            }

            .article-header {
                flex-direction: column;
            }

            .article-section,
            .research-question-card {
                padding: var(--spacing-lg);
            }

            .orb {
                display: none;
            }
        }

        /* ACCESSIBILITY */
        @media (prefers-reduced-motion: reduce) {
            *,
            *::before,
            *::after {
                animation-duration: 0.01ms !important;
                animation-iteration-count: 1 !important;
                transition-duration: 0.01ms !important;
            }
        }
    </style>
</head>
<body>
    <div class="scroll-indicator" id="scrollIndicator"></div>
    <div class="cosmic-bg"></div>
    
    <div class="orbs-container">
        <div class="orb orb-1"></div>
        <div class="orb orb-2"></div>
        <div class="orb orb-3"></div>
    </div>

    <header class="site-header">
        <div class="header-content">
            <h1 class="site-title">You Agreed.</h1>
            <nav class="nav-links">
                <a href="#" class="nav-link active">Research Foundation</a>
                <a href="projects.html" class="nav-link">Related Projects</a>
                <a href="design-guide.html" class="nav-link">Design Guide</a>
                <a href="exhibition-showcase.html" class="nav-link">Conceptual Showcase</a>
                <a href="exhibition-layout.html" class="nav-link">Exhibition Layout</a>
                <a href="References.html" class="nav-link">Reference List</a>
            </nav>
        </div>
    </header>

    <section class="hero-section">
        <h2 class="hero-title">Research Foundation</h2>
        <p class="hero-subtitle">Theoretical Framework & Critical Literature Review</p>
    </section>

    <main class="main-container">
        <!-- PRIMARY RESEARCH QUESTION -->
        <div class="research-question-card">
            <div class="research-question-label">Primary Research Question</div>
            <div class="research-question-text">
                Can informed consent mechanisms designed for reversible data collection (cookies, tracking) function ethically when applied to irreversible cognitive extraction through AI systems?
            </div>
        </div>

        <!-- ARTICLE 1: ZUBOFF -->
        <div class="article-section" style="--section-color-1: #ec4899; --section-color-2: #f97316; animation-delay: 0.8s;">
            <div class="article-header">
                <div class="article-number">1</div>
                <div class="article-meta">
                    <h3 class="article-title">"Surveillance Capitalism or Democracy? The Death Match of Institutional Orders and the Politics of Knowledge in Our Information Civilisation"</h3>
                    <div class="article-details">
                        <strong>Author:</strong> Shoshana Zuboff<br>
                        <strong>Year:</strong> 2022<br>
                        <strong>Published:</strong> Organization Theory, Vol 3(3)
                    </div>
                </div>
            </div>
            <div class="article-content">
                <p>The first article which critically contextualised my project was Zuboff's 2022 extension of her surveillance capitalism theory, mapping the institutional development of what she terms "the commodification of human behaviour operationalised in the secret massive-scale extraction of human-generated data." Zuboff argues that surveillance capitalism has evolved through four distinct stages, each building scaffolding for the next through "novel economic operations, governance carve-outs, and fresh social harms." Of particular significance to my project is her analysis of Stage Two: "The Concentration of Computational Knowledge Production and Consumption," where she documents how massive-scale behavioural data becomes transformed into computational knowledge through AI and machine learning systems. She observes that these systems "cannot perform without an ever-expanding diet of massive-scale human generated data," establishing what she calls "economies of learning" where "scale begets learning through the accumulation of data and increases competitive advantage." This creates a self-reinforcing cycle: more data enables better AI, which enables more sophisticated extraction, which generates more data. Crucially, Zuboff identifies epistemic inequality as the defining social harm of Stage Two (the growing gap between the many and the few now defined by the difference between what I can know and what can be known about me). Knowledge is "scraped from human lives but accrues to the improvement of the few, not the many," fundamentally altering what she calls the division of learning in society.</p>

                <p>Zuboff's staged framework provides the theoretical foundation for my core argument about consent inadequacy across technological evolution. Her Stage One (behavioural data extraction through cookies and tracking) represents the context for which current consent frameworks like GDPR were designed, frameworks built around assumptions of reversibility and behavioural data. However, my project identifies and critiques what I term the cognitive extraction phase, an evolution that builds directly on Zuboff's Stage Two logic. Where Stage Two focuses on the production of AI knowledge from aggregated behavioural data, my work examines the extraction of cognitive data directly from individual interactions with AI systems themselves. When users engage in extended conversations with ChatGPT, they're not merely generating behavioural data like clicks or timestamps; they're disclosing reasoning processes, emotional vulnerabilities, problem-solving methods, and communication styles (what I describe as "the infrastructure of selfhood itself"). This represents a qualitative shift from tracking what people do to extracting how people think, yet we continue applying the same consent theatre that failed at Stage One. Zuboff's observation that "each stage creates the conditions and constructs the scaffolding for the next" validates my argument that the concentration of computational knowledge she documents has enabled this next evolution: AI systems sophisticated enough to extract cognitive data through natural language interaction, creating an irreversible form of extraction that cookie-era consent frameworks cannot ethically govern.</p>

                <p>The article helped frame my understanding that epistemic inequality intensifies when extraction becomes cognitive rather than merely behavioural. Zuboff's concept of the gap between "what I can know and what can be known about me" takes on new urgency in conversational AI contexts, where the asymmetry isn't just about aggregated population-level insights but about permanent cognitive patterns extracted from individual conversations and embedded into AI models. Her warning that "it is possible to have surveillance capitalism, and it is possible to have a democracy. It is not possible to have both" underscores why my installation exists (to make this abstract theoretical critique viscerally felt). By creating the conditions where users experience the exact moment when legal consent diverges from genuine understanding, <em>You Agreed</em> operationalises Zuboff's analysis as experiential education, revealing that the "secret massive-scale extraction" she identifies has evolved to encompass not just behavioural surplus but cognitive architecture itself, extracted through the same broken consent mechanisms designed for an earlier, less invasive stage of surveillance capitalism.</p>
            </div>
        </div>

        <!-- ARTICLE 2: NISSENBAUM -->
        <div class="article-section" style="--section-color-1: #3b82f6; --section-color-2: #8b5cf6; animation-delay: 1s;">
            <div class="article-header">
                <div class="article-number">2</div>
                <div class="article-meta">
                    <h3 class="article-title">"A Contextual Approach to Privacy Online"</h3>
                    <div class="article-details">
                        <strong>Author:</strong> Helen Nissenbaum<br>
                        <strong>Year:</strong> 2011<br>
                        <strong>Published:</strong> Daedalus, Vol 140(4): 32-48
                    </div>
                </div>
            </div>
            <div class="article-content">
                <p>The second article which critically contextualised my project was Nissenbaum's 2011 critique of the notice-and-consent model that underpins contemporary privacy policy, particularly online. Nissenbaum argues that "proposals to improve and fortify notice-and-consent, such as clearer privacy policies and fairer information practices, will not overcome a fundamental flaw in the model, namely, its assumption that individuals can understand all facts relevant to true choice at the moment of pair-wise contracting between individuals and data gatherers." Her analysis reveals that the transparency-and-choice approach (where users are informed of data collection practices and given the option to engage or disengage) has demonstrably failed, not due to poor implementation but due to structural inadequacy. She documents how privacy policies are "long, abstruse, and legalistic," adding to the "unrealistic burden of checking the respective policies of the websites we visit." Evidence shows that "people do not read privacy policies, do not understand them when they do, and realistically could not read them even if they wanted to." Crucially, Nissenbaum positions this not as "weakness of the will" but as a fundamental design failure: the consent model operates as "a procedural mechanism divorced from the particularities of relevant online activity." Her alternative framework of contextual integrity proposes that privacy protection should be grounded in context-specific norms that govern what information can be collected, with whom it can be shared, and under what conditions, rather than relying on individual comprehension and choice at the moment of transaction.</p>

                <p>Nissenbaum's demolition of notice-and-consent provides the theoretical justification for my project's core critique. If the consent model was already structurally inadequate for Stage 1 surveillance capitalism (behavioural tracking through cookies and online activity), then its application to conversational AI represents a catastrophic category error. The "fundamental flaw" she identifies (that individuals cannot "understand all facts relevant to true choice at the moment of contracting") intensifies exponentially when the data being extracted is cognitive rather than behavioural. Her observation that even experts struggle to piece together "what information is captured, where it is sent, and how it is used" in online behavioural advertising takes on new dimensions with AI training data, where the mechanisms of model training, the permanence of cognitive pattern extraction, and the impossibility of data removal remain opaque even to many AI researchers. The consent theatre I critique in <em>You Agreed</em> (dense terms of service with buried consequences, the assumption that scrolling equals comprehension, the fiction that clicking "I Agree" constitutes informed decision-making) represents exactly the procedural mechanism Nissenbaum warns against, now applied to an even more consequential form of extraction. When OpenAI's ChatGPT terms run to thousands of words and Anthropic's privacy policies demand substantial legal literacy, we're witnessing the same broken framework Nissenbaum documented in 2011, now governing data that cannot be reversed, deleted, or opted out of retroactively.</p>

                <p>The article helped frame my understanding that improving consent mechanisms through better interface design or clearer language (what Nissenbaum calls "correctives" from "critical adherents") cannot solve the underlying structural problem. Her argument that notice-and-consent fails because it's "divorced from the particularities of relevant online activity" validates my installation's methodology: rather than trying to perfect consent interfaces, <em>You Agreed</em> demonstrates why the entire framework is inadequate by making users experience the gap between legal consent and genuine understanding. Nissenbaum's contextual integrity framework suggests that privacy norms should reflect the specific context of information flow, but current AI platforms apply cookie-era consent to cognitive extraction contexts, ignoring the qualitative difference between reversible behavioural tracking and irreversible cognitive data incorporation into model training. By creating the conditions where visitors encounter authentic terms of service, scroll past buried clauses they don't read, click "I Agree" to access desired functionality, and only later discover what they've actually consented to, my project operationalises Nissenbaum's critique: it proves that the procedural mechanism itself (regardless of implementation quality) cannot function ethically when the stakes involve permanent extraction of cognitive data rather than temporary collection of behavioural information.</p>
            </div>
        </div>

        <!-- ARTICLE 3: CONVERSATIONAL AI PRIVACY -->
        <div class="article-section" style="--section-color-1: #8b5cf6; --section-color-2: #ec4899; animation-delay: 1.2s;">
            <div class="article-header">
                <div class="article-number">3</div>
                <div class="article-meta">
                    <h3 class="article-title">"User Privacy Harms and Risks in Conversational AI: A Proposed Framework"</h3>
                    <div class="article-details">
                        <strong>Authors:</strong> Multiple researchers<br>
                        <strong>Year:</strong> 2024 (February)<br>
                        <strong>Published:</strong> arXiv:2402.09716v1
                    </div>
                </div>
            </div>
            <div class="article-content">
                <p>The third article which critically contextualised my project was a 2024 empirical study that developed a comprehensive framework for understanding privacy harms and risks specifically in text-based AI chatbot interactions. Through semi-structured interviews with 13 participants interacting with two AI chatbots, the researchers identified 9 distinct privacy harms and 9 privacy risks, applying and extending Solove's taxonomy to the conversational AI context. The study revealed that "user manipulation through the chatbots' algorithm raises privacy concerns" and documented how design elements such as "high empathy, emotional engagement, trust, human-likeness, personalisation, and self-disclosure can potentially compromise user privacy." Critically, the research found that these conversational systems "have increasingly gathered substantial amounts of personal information without transparent user consent," creating conditions where users "voluntarily disclose personal information to AI chatbots" due to the manipulative potential of conversational design. The framework categorises self-disclosure as a distinct privacy harm, documenting how the conversational interface itself (through its human-like qualities and emotional engagement mechanisms) prompts users to reveal information they might not disclose through traditional interfaces. The study emphasises that "there is a significant gap in comprehensive laws and regulations governing these interfaces in both the United States and the EU," underscoring that current privacy frameworks remain inadequate for addressing the unique risks posed by conversational AI systems that continuously collect data through natural language interaction.</p>

                <p>This empirical framework provides crucial evidence for my project's central argument that people are actively disclosing cognitive and intimate data to conversational AI systems without comprehending the privacy implications. The researchers' documentation of self-disclosure as a privacy harm validates my focus on ChatGPT conversations as a site of cognitive extraction (users aren't merely generating behavioural data through clicks and usage patterns, they're actively revealing "personal information," "emotional attributes," and the reasoning processes that characterise how they think). The study's identification of manipulation as a privacy concern directly supports my critique of consent theatre: when "chatbots' algorithm raises privacy concerns" and design features like emotional engagement and human-likeness "potentially compromise user privacy," we're witnessing exactly the kind of structural deception that makes notice-and-consent frameworks inadequate. The finding that users "exhibit hesitation in AI chatbot decision-making, possibly due to uncertainty about data usage" demonstrates the gap between clicking "I Agree" and genuinely understanding consequences (the exact gap my installation makes visceral through the countdown experience). Moreover, the researchers' emphasis on trust as "a bridge between technology and the user, making users more vulnerable to revealing personal information" explains why conversational AI represents a qualitatively different privacy threat: the conversational interface itself, designed to feel empathetic and human-like, manipulates users into disclosure in ways that traditional web forms do not.</p>

                <p>The article helped frame my understanding that the privacy crisis in conversational AI isn't hypothetical or future-facing (it's empirically observable right now through user behaviour and documented harms). The researchers' finding that "chatbots have increasingly gathered substantial amounts of personal information without transparent user consent" provides concrete evidence that Stage 2.5 cognitive extraction is already operational, not merely theoretical. Their identification of secondary use as a privacy harm (where data collected for one purpose, improving chatbot responses, gets used for another, training AI models) maps directly onto my critique of how ChatGPT conversations become permanent training data despite users believing they're simply having helpful interactions. The framework's emphasis on exclusion as a privacy harm, occurring when "a person is excluded from receiving notices about their personal data" due to "lack of transparency and accountability," validates my installation's method of revealing buried consent clauses: current systems exclude users from genuine understanding while maintaining the legal fiction of informed consent. By demonstrating that conversational design features systematically compromise privacy through manipulation and prompted self-disclosure, this empirical framework provides the evidence base for my argument that applying cookie-era consent to conversational AI isn't merely inadequate (it's actively enabling a new form of extraction that exploits the human tendency to trust and disclose to entities that present as empathetic conversational partners).</p>
            </div>
        </div>

    </main>

    <footer class="site-footer">
        <p>You Agreed. | COMM3705 Digital Media Project | University of Leeds | May 2026</p>
    </footer>

    <script>
        // SCROLL INDICATOR
        function updateScrollIndicator() {
            const winScroll = document.documentElement.scrollTop;
            const height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
            const scrolled = (winScroll / height) * 100;
            document.getElementById('scrollIndicator').style.width = scrolled + '%';
        }

        let ticking = false;
        window.addEventListener('scroll', () => {
            if (!ticking) {
                window.requestAnimationFrame(() => {
                    updateScrollIndicator();
                    ticking = false;
                });
                ticking = true;
            }
        }, { passive: true });

        // INTERSECTION OBSERVER FOR ANIMATIONS
        const observerOptions = {
            threshold: 0.1,
            rootMargin: '0px 0px -100px 0px'
        };

        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.opacity = '1';
                    entry.target.style.transform = 'translateY(0)';
                }
            });
        }, observerOptions);

        document.querySelectorAll('.article-section').forEach(el => {
            observer.observe(el);
        });
    </script>
</body>
</html>